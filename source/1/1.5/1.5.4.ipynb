{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 框架基础(Pytorch)\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "PyTorch是一个建立在Torch库之上的Python包，旨在加速深度学习应用。PyTorch提供了一种类似于NumPy的抽象方法来表征张量（或多维数组），利用GPU来加速训练；同时，PyTorch采用动态计算图结构，可低延迟甚至是零延迟的改变网络行为。\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "PyTorch主要由4个包组成：\n",
    "\n",
    "- **torch**：与NumPy类似的通用包，可将张量类型转换为可在GPU上进行计算的类型。\n",
    "\n",
    "- **torch.autograd**：能构建计算图形并自动获取梯度的包。\n",
    "\n",
    "- **torch.nn**：具有共享层和损失函数等功能的神经网络操作包。\n",
    "\n",
    "- **torch.optim**：具有通用优化算法的包。\n",
    "\n",
    "1．导入torch包\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "PyTorch深度学习框架在导入时其包的名称为torch，如下示例可查看当前框架的版本号。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "# 导入PyTorch包，以下代码均含此行，本书为节省篇幅省略此行\n",
    "import torch\n",
    "\n",
    "# 查看其版本号，不同电脑版本不同其结果可能不同\n",
    "print(torch.__version__)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 创建张量(Tensor)\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "几何代数中定义的张量（Tensor）是基于向量和矩阵的推广，可以将标量视为零阶张量，向量视为一阶张量，矩阵视为二阶张量。标量是一个单独的数；向量是一列数，且这些数是有序排列的；矩阵是二维数组，其中每一个元素被两个索引所确定。\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "张量是一个可用来表示在一些矢量、标量和其他张量之间的线性关系的多线性函数，可理解为一个n维数值阵列。通俗来讲，可以将任意一张彩色图片表示为一个三阶张量，其三个维度分别是图片的高度、宽度和色彩数据；同时，也可以用四阶张量表示一个包含多张图片的数据集，其四个维度分别是图片在数据集中的编号与图片的高度、宽度和色彩数据。\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "PyTorch的Tensor可以是零维（又称为标量或一个数）、一维（行或列）、二维（又称矩阵）及多维的数组。与NumPy中的ndarray相似，其最大区别是NumPy会把ndarray放在CPU中进行运算，而PyTorch的Tensor会放在GPU中进行加速运算。其中常见创建Tensor的方法如表1-3所示（*size表示可以接收多个参数）。\n",
    "\n",
    ":::{table} 表 1-3 创建Tensor 的常见方法\n",
    ":align: center\n",
    ":widths: grid\n",
    "\n",
    "|函数| 功能                             |\n",
    "|---|--------------------------------|\n",
    "|tensor(*size)| \t直接从参数构造一个张量，支持List、Numpy数组    |\n",
    "|eye(row, column)\t| 创建指定行数、列数的二维Tensor             |\n",
    "|linspace(stat,end,steps)\t| 将区间[start,end)均分成steps份        |\n",
    "|logspace(stat,end,steps)\t| 将区间[10^start,10^end)均分成steps份  |\n",
    "|rand/randn(*size)\t| 生成[0,1)均匀分布/标准正态分布数据           |\n",
    "|ones(*size)\t| 返回指定shape的张量，元素初始为1            |\n",
    "|zeros(*size)\t| 返回指定shape的张量，元素初始为0            |\n",
    "|ones_like(t)\t| 返回与t的shape相同的张量，且元素初始为1        |\n",
    "|zeros_like(t)\t| 返回与t的shape相同的张量，且元素初始为0        |\n",
    "|arrange(stat,end,step)\t| 在区间[start,end)上以间隔step生成一个序列张量 |\n",
    "|from_Numpy(ndarray)\t| 从ndarray创建一个Tensor             |\n",
    ":::\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "以下示例将演示如何用torch创建张量：\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)\n",
      "tsr2 size: torch.Size([2])\n",
      "xx size torch.Size([5, 6])\n",
      "yy size torch.Size([5, 6])\n",
      "zz size torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一维Tensor\n",
    "tsr1 = torch.tensor(np.arange(1, 10, 1))\n",
    "print(tsr1)\n",
    "# 创建指定形状的Tensor\n",
    "tsr2 = torch.tensor((2, 3))\n",
    "print(\"tsr2 size:\", tsr2.size())\n",
    "# 创建元素全为0的5×6二维Tensor\n",
    "xx = torch.zeros(5, 6)\n",
    "print(\"xx size\", xx.size())\n",
    "# 创建空（杂乱数据值）的5×6二维Tensor\n",
    "yy = torch.empty(5, 6)\n",
    "print(\"yy size\", yy.size())\n",
    "# 创建随机数组成的5×6二维Tensor，可指定生成方式和值范围\n",
    "zz = torch.rand(5, 6)\n",
    "\n",
    "print(\"zz size\", zz.size())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Tensor 基本计算\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "Tensor可以进行加减乘除等基本运算，其运算及相应的显示结果如下所示。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx+yy= tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "xx-yy= tensor([[-1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1., -1., -1., -1., -1.]])\n",
      "xx*yy= tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "xx/yy= tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建元素全为0的5×6二维Tensor\n",
    "xx = torch.zeros(5, 6)\n",
    "# 创建元素全为1的5×6二维Tensor\n",
    "yy = torch.ones(5, 6)\n",
    "\n",
    "print(\"xx+yy=\", xx + yy)\n",
    "print(\"xx-yy=\", xx - yy)\n",
    "print(\"xx*yy=\", xx * yy)\n",
    "print(\"xx/yy=\", xx / yy)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Tensor 形状改变\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "张量的形状可以通过reshape操作改变，代码示例如下。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx size: torch.Size([5, 6])\n",
      "yy size: torch.Size([30])\n",
      "zz size: torch.Size([5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "xx = torch.zeros(5, 6)\n",
    "print(\"xx size:\", xx.size())\n",
    "yy = xx.reshape(30)\n",
    "print(\"yy size:\", yy.size())\n",
    "zz = xx.reshape(5, 3, 2)\n",
    "print(\"zz size:\", zz.size())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "&ensp;&ensp;&ensp;&ensp;\n",
    "修改形状的常用函数如表1-4所示。\n",
    "\n",
    ":::{table} 表 1-4 Tensor 修改形状常用函数\n",
    ":align: center\n",
    ":widths: grid\n",
    "\n",
    "|函数| 说明       |\n",
    "|-----|----------|\n",
    "|size()\t|返回张量的shape属性值，与函数shape（0.4版新增）等价|\n",
    "|numel(input)\t|计算Tensor的元素个数|\n",
    "|view(*shape)\t|修改Tensor的shape，与Reshape（0.4版新增）类似，但View返回的对象与源Tensor共享内存，修改一个，另一个同时修改。Reshape将生成新的Tensor，而且不要求源Tensor是连续的。view(-1)展平数组|\n",
    "|resize\t|类似于view，但在size超出时会重新分配内存空间|\n",
    "|item\t|若Tensor为单元素，则返回Python的标量|\n",
    "|unsqueeze\t|在指定维度增加一个“1”|\n",
    "|squeeze\t|在指定维度压缩一个“1”|\n",
    "\n",
    ":::\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. 与Numpy 之间的转换\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "PyTorch中的张量可以和NumPy之间的表示进行转换，其示例代码如下。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yy type= <class 'numpy.ndarray'>\n",
      "zz type= <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "xx = torch.zeros(5, 6)\n",
    "yy = xx.numpy()\n",
    "print(\"yy type=\", type(yy))\n",
    "zz = torch.from_numpy(yy)\n",
    "print(\"zz type=\", type(zz))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. 张量中元素访问\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "和Python数组一样，张量也可以通过索引和切片访问元素，其中常见的元素访问函数如表1-5所示。\n",
    "\n",
    ":::{table} 表1-5 常用的元素访问函数\n",
    ":align: center\n",
    ":widths: grid\n",
    "\n",
    "|函数\t|说明|\n",
    "|------|-------|\n",
    "|index_select(input,dim,index)\t|在指定维度上选择一些行或列|\n",
    "|nonzero(input)\t|获取非0元素的下标|\n",
    "|masked_select(input,mask)\t|使用二元值进行选择|\n",
    "|gather(input,dim,index)\t|在指定维度上选择数据，输出的形状与index（index的类型必须是LongTensor类型的）一致|\n",
    "|scatter_(input,dim,index,src)\t|为gather的反操作，根据指定索引补充数据|\n",
    ":::\n",
    "Tensor元素访问示意如下：\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]], dtype=torch.int32)\n",
      "tensor([[ 1,  2],\n",
      "        [ 7,  8],\n",
      "        [13, 14],\n",
      "        [19, 20],\n",
      "        [25, 26]], dtype=torch.int32)\n",
      "tensor([[1, 2],\n",
      "        [7, 8]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "xx = torch.from_numpy(np.arange(0, 30, 1)).reshape(5, 6)\n",
    "print(xx[0:2, :])\n",
    "print(xx[:, 1:3])\n",
    "print(xx[0:2, 1:3])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. 广播机制\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "PyTorch的广播机制与NumPy类似，使用示例如下所示。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsrA1 size: torch.Size([5, 1])\n",
      "tsrB1 size: torch.Size([3])\n",
      "tsrC1 size: torch.Size([5, 3])\n",
      "tsrC2 size: torch.Size([5, 3])  equal: tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "arrA = np.arange(0, 50, 10).reshape(5, 1)\n",
    "arrB = np.arange(0, 3, 1)\n",
    "tsrA1 = torch.from_numpy(arrA)\n",
    "print(\"tsrA1 size:\", tsrA1.size())\n",
    "tsrB1 = torch.from_numpy(arrB)\n",
    "print(\"tsrB1 size:\", tsrB1.size())\n",
    "tsrC1 = tsrA1 + tsrB1\n",
    "print(\"tsrC1 size:\", tsrC1.size())\n",
    "# 广播的实现过程\n",
    "tsrB2 = tsrB1.unsqueeze(0)\n",
    "tsrA2 = tsrA1.expand(5, 3)\n",
    "tsrB3 = tsrB2.expand(5, 3)\n",
    "tsrC2 = tsrA2 + tsrB3\n",
    "print(\"tsrC2 size:\", tsrC2.size(), \" equal:\", tsrC2 == tsrC1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. 逐元素运算\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "PyTorch的逐元素运算操作与NumPy类似，其常见的函数如表1-6所示。\n",
    "\n",
    ":::{table} 表1-6 常见逐元素运算函数\n",
    ":align: center\n",
    ":widths: grid\n",
    "\n",
    "|函数\t|说明|\n",
    "|------|------|\n",
    "|abs/add\t|绝对值 / 加法|\n",
    "|addcdiv(t, v, t1, t2)\t|t1与t2按元素逐个相除后，乘v加t|\n",
    "|addcmul(t, v, t1, t2)\t|t1与t2按元素逐个相乘后，乘v加t|\n",
    "|ceil/floor\t|向上取整 / 向下取整|\n",
    "|clamp(t, min, max)\t|将张量元素限制在指定区间[min,max]|\n",
    "|exp/log/pow\t|指数 / 对数 / 幂|\n",
    "|mul(或 *)/neg\t|逐元素乘法 / 取反|\n",
    "|sigmoid/tanh/softmax\t|激活函数|\n",
    "|sign/sqrt\t|取符号 / 开根号|\n",
    ":::\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "逐元素运算函数使用示例如下所示：\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  6,  8, 10], dtype=torch.int32)\n",
      "tensor([[ 31,  42,  53,  64],\n",
      "        [ 61,  82, 103, 124],\n",
      "        [ 91, 122, 153, 184],\n",
      "        [121, 162, 203, 244]], dtype=torch.int32)\n",
      "tensor([1, 2, 3, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "tsrA1 = torch.from_numpy(np.arange(1, 5, 1))\n",
    "tsrA2 = torch.from_numpy(np.arange(1, 5, 1).reshape(4, 1))\n",
    "tsrA3 = torch.from_numpy(np.arange(3, 7, 1))\n",
    "print(torch.add(tsrA1, tsrA3))\n",
    "print(torch.addcmul(tsrA1, tsrA2, tsrA3, value=10.0))\n",
    "print(torch.clamp(tsrA1, 0, 3))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. 数据预处理\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "深度学习需要处理大量数据，而在此过程中一般需进行数据预处理，代码示例如下，原始数据及处理后的数据结果如图1-26所示。\n",
    "\n",
    ":::{figure-md}\n",
    "<img src=\"../../_static/1/1.5/1-26.png\" alt=\"图 1-26 原始数据及处理后的数据结果\">\n",
    "\n",
    "图 1-26 原始数据及处理后的数据结果\n",
    ":::\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year PCCount       GDP\n",
      "0  2000     500    298900\n",
      "1  2006     NAN    498900\n",
      "2  2008    1500    586900\n",
      "3  2016     NAN  12356789\n",
      "4  2020   19500   2335600\n",
      "5  2022     NAN   5668500\n",
      "   Year PCCount\n",
      "0  2000     500\n",
      "1  2006     NAN\n",
      "2  2008    1500\n",
      "3  2016     NAN\n",
      "4  2020   19500\n",
      "5  2022     NAN\n",
      "0      298900\n",
      "1      498900\n",
      "2      586900\n",
      "3    12356789\n",
      "4     2335600\n",
      "5     5668500\n",
      "Name: GDP, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def createDataFile():\n",
    "    os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "    dataFile = os.path.join('..', 'data', 'demo_data.csv')\n",
    "    with open(dataFile, 'w') as f:\n",
    "        f.write('Year,PCCount,GDP\\n')\n",
    "        f.write('2000,500,298900\\n')\n",
    "        f.write('2006,NAN,498900\\n')\n",
    "        f.write('2008,1500,586900\\n')\n",
    "        f.write('2016,NAN,12356789\\n')\n",
    "        f.write('2020,19500,2335600\\n')\n",
    "        f.write('2022,NAN,5668500\\n')\n",
    "        return dataFile\n",
    "\n",
    "\n",
    "def readDataFile(dataFile):\n",
    "    data = pd.read_csv(dataFile)\n",
    "    print(data)\n",
    "    inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "    # 第[1,3)列与第3列内容分别存到两变量\n",
    "    inputs = inputs.fillna(inputs.mean)\n",
    "    print(inputs)\n",
    "    print(outputs)\n",
    "\n",
    "\n",
    "dataFile = createDataFile()\n",
    "readDataFile(dataFile)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Autograd 自动求导\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "现今大部分深度学习框架（PyTorch、TensorFlow等）都具备自动求导功能，在PyTorch中是靠torch.autograd包实现自动求导。该包为张量操作提供了自动求导功能，其包括torch.Function和autograd两个主要类。\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "在自动求导的整个过程中，PyTorch采用计算图的形式进行组织，该计算图为动态图，且在每次前向传播时将重新构建，其中自动求导包括的主要步骤如下：\n",
    "\n",
    "- （1）创建叶子节点的Tensor，使用requires_grad参数指定是否记录对其进行的操作，以便之后利用backward()方法进行梯度求解。requires_grad参数默认值为False，如果要对其求导需设置为True，然后与之有依赖关系的节点会自动变为True。\n",
    "\n",
    "- （2）可利用requires_grad ()方法修改Tensor的requires_grad属性，同时调用.detach()或with torch.no_grad()：此时将不再计算张量的梯度，也不跟踪张量的历史记录。\n",
    "\n",
    "- （3）通过运算创建的非叶子节点Tensor，会自动被赋予grad_fn属性，该属性表示梯度函数，而叶子节点的grad_fn值为None。\n",
    "\n",
    "- （4）对得到的Tensor执行backward()函数，此时自动计算各变量的梯度，并将累加结果保存到grad属性中，一旦计算完成后，非叶子节点的梯度自动释放。\n",
    "\n",
    "- （5）其中backward()函数接收的参数应和调用本函数的Tensor的维度相同（或是可广播为相同的维度）。如果求导的Tensor为标量，则backward()中的参数可省略。\n",
    "\n",
    "- （6）反向传播的中间缓存会被清空，如果需要进行多次反向传播，需要指定backward中的参数retain_graph值为True，多次反向传播时，梯度会累加。\n",
    "\n",
    "- （7）非叶子节点的梯度，在backward()函数调用后即被清空。\n",
    "\n",
    "- （8）可用torch.no_grad()包裹代码块的方式，以阻止autograd去跟踪requires_grad值为True的张量的历史记录。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "11. Backward反向传播\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "在PyTorch中的backward()函数，通过反向传播过程可自动计算各叶子节点的梯度，同时其叶子节点的梯度值将累加到grad属性中，非叶子节点的计算操作将记录在grad_fn属性中。此处以z=wx+b（中间变量y=wx）为例介绍其实现的主要步骤。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x,w,b的require_grad值为：False,True,True\n",
      "y，z的requires_grad值分别为：True,True\n",
      "x,w,b,y,z的叶子节点属性：True,True,True,False,False\n",
      "x,w,b的grad_fn属性：None,None,None\n",
      "y,z的叶子节点属性：False,False\n",
      "tensor([2.1398], grad_fn=<AddBackward0>)\n",
      "w,b,x的梯度分别为:tensor([2.]),tensor([1.]),None\n",
      "非叶子节点y,z的梯度分别为:None,None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2])\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "y = torch.mul(w, x)\n",
    "z = torch.add(y, b)\n",
    "\n",
    "# x，w，b叶子节点的值\n",
    "print(\"x,w,b的require_grad值为：{},{},{}\".format(x.requires_grad,\n",
    "                                                w.requires_grad,\n",
    "                                                b.requires_grad))\n",
    "\n",
    "# 查看叶子节点、非叶子节点的其他属性：\n",
    "print(\"y，z的requires_grad值分别为：{},{}\".format(y.requires_grad,\n",
    "                                                z.requires_grad))\n",
    "# 非叶子节点的requires_grad值\n",
    "# 说明：因与w，b有依赖关系，故y，z的requires_grad属性也是：True,True\n",
    "print(\"x,w,b,y,z的叶子节点属性：{},{},{},{},{}\".format(x.is_leaf,\n",
    "                                                      w.is_leaf,\n",
    "                                                      b.is_leaf,\n",
    "                                                      y.is_leaf,\n",
    "                                                      z.is_leaf))\n",
    "# 查看各节点是否叶子节点\n",
    "print(\"x,w,b的grad_fn属性：{},{},{}\".format(x.grad_fn,\n",
    "                                           w.grad_fn,\n",
    "                                           b.grad_fn))\n",
    "# 叶子节点的grad_fn属性\n",
    "# 说明：因x，w，b为用户创建的，故grad_fn属性为None\n",
    "print(\"y,z的叶子节点属性：{},{}\".format(y.grad_fn == None,\n",
    "                                       z.grad_fn == None))\n",
    "# y，z是否为叶子节点\n",
    "# 自动求导，实现梯度的反向传播：\n",
    "z.backward()\n",
    "# 基于z张量进行梯度反向传播，如果需要多次使用backward\n",
    "# 需要修改参数retain_graph为True，此时梯度是累加的\n",
    "print(z)\n",
    "print(\"w,b,x的梯度分别为:{},{},{}\".format(w.grad,\n",
    "                                          b.grad, x.grad))\n",
    "# 说明：x是叶子节点但它无须求导，故其梯度为None\n",
    "print(\"非叶子节点y,z的梯度分别为:{},{}\".format(y.retain_grad(),\n",
    "                                               z.retain_grad()))\n",
    "# 说明：当执行backward之后，非叶子节点的梯度会自动清空"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

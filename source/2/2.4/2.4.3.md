# 时序差分学习方法

&ensp;&ensp;&ensp;&ensp;
基于贝尔曼方程的时序差分学习（Temporal-Difference Learning, TD Learning）是经典的预测学习算法之一；许多经典的无模型（Model-free）强化学习算法，如Q-learning算法、SARSA算法等都是基于时序差分学习方法。强化学习算法的学习过程中，一般要经历智能体从初始状态到终止状态的多轮学习，以期学习到全局最优策略。

&ensp;&ensp;&ensp;&ensp;
在每一轮的学习过程中，对于无模型的强化学习，目前多采用时序差分学习方法进行迭代，即在每一个（或几个）状态转移完成后，都更新一遍值函数的估计值（因为未最终收敛，所以统称为估计值）。在具体计算中，给定策略后，时序差分学习方法会针对出现的当前状态St更新值函数V，包括状态值函数和动作值函数。

&ensp;&ensp;&ensp;&ensp;
在t时刻，智能体状态为St，然后智能体根据策略做一个动作后可进入下一个状态St+1，并得到一个环境反馈奖励Rt+1。在学习过程中，对任意一个状态St+1，都有一个其对应的V(St+1)。对于智能体的St状态，可使用Rt+1和V(St+1)对St状态的值函数（本轮学习）进行更新：

$$

V(S_t)\leftarrow V(S_t)+ \alpha[R_{t+1} + \gamma V(S_{t+1})-V(S_t)] \tag {2-70}

$$

&ensp;&ensp;&ensp;&ensp;
上式是强化学习领域对时序差分学习的更新方法进行的一种规定。
$R_{t+1}+γV(S_{t+1})-V(S_t)$ 是当前轮值函数估计值与上一轮的差值，再将此差值乘以一个小于1的学习率α并加到上一轮的$V(S_t)$上，得到一个和，然后将这个和赋值给当前轮的$V(S_t)$，这就是值函数每一个状态都进行更新的过程。

&ensp;&ensp;&ensp;&ensp;
可见，时序差分学习方法的手段是，在某一轮学习中，当前状态St的值函数估计值$V(S_t)$更新，是利用其下一状态St+1的环境反馈奖励$R_{t+1}$和值函数估计值$V(S_{t+1})$。

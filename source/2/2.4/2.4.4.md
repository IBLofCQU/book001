# 采用Q-learning 算法进行智能体路径规划

1. Q-leaning算法介绍

Q-learning是强化学习中的一种基本的基于价值的算法，它以上述所说的贝尔曼最优方程、最优价值函数和时序差分学习为基础。智能体通过与环境的交互和学习，调整智能体本身的行为以适应环境。Q-leaning的伪代码如下表所示。Q(s,a)是在某个状态s下，选择动作a能够获得的奖励期望，环境会根据智能体的动作反馈相应的奖励R，Q-Table则是Q(s,a)奖励期望的集合表格。在Q-table中，表格的列为可选择的动作a，表格的行为不同的状态s。Q-table用于指引智能体在不同的状态s下，选择最合适的动作a。Q-leaning算法的主要思想就是将状态s与动作a构建成一张Q-table来存储Q(s,a)，然后根据Q(s,a)来选取能够获得最大奖励的动作。

<div class="show-mid">表 2-2 Q-learning 算法</div>

***

输入：学习率α∈[0,1]、学习次数 episode、折扣因子γ∈[0,1]

输出：Q*
***

```{code-block}
:linenos:

对所有s∈S、a∈A，初始化所有状态动作对下的表项Q(s,a), Q(terminal)=0
for i＜1 to episode do
    初始化S
    while S != terminal
        根据现有的Q(s,)、当前状态(s)和对应的策略，选择一个动作(a)
        执行动作(a)并观测产生的状态(s’)和奖励(r’)
        更新Q(s,a): Q(s,a)   Q(s,a)+α[rt+1+γmaxQ’(s’,a’)-Q(s,a)]
        令s=s’
    end while
```

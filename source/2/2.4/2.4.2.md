# 强化学习理论基础


强化学习是机器学习的重要分支，主要用于解决序列决策（Sequential Decision）问题。在强化学习中，智能体与环境交互的过程可以用马尔科夫决策过程（Markov decision process，MDP）进行建模。马尔科夫决策过程是序列决策过程的数学模型，一般用于具备马尔科夫性的环境中，是智能推荐、强化学习、自动化控制、资源管理的基础理论框架。我们下面将按照逻辑顺序，对马尔科夫决策过程的理论基础进行介绍。

1. 马尔科夫性（Markov Property）和状态转移概率矩阵（State Transition Probability Matrix）


在离散马尔可夫链中，智能体的所有状态向其它状态转移的概率可以组成一个状态转移概率矩阵P，简称转移矩阵。马尔科夫性指的是若未来的状态仅与当前的状态有关，与其他时刻的状态独立，即转化到未来下一时刻的状态St+1的概率仅与当前时刻的状态St有关，与之前的状态无关，用状态转移的概率公式表述如下：

$$

P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...,S_t] \tag {2-52}

$$

2. 马尔科夫过程（Markov Process）


马尔科夫过程MP又称马尔科夫链，是一个包含马尔科夫性的随机过程，可以用一个二元组(S, P)来描述，其中S={s1, s2, s3, ..., sn}，为一个有限状态集合，P为状态转移概率矩阵，公式如下：

$$

P_{ss}=P[S_{t+1}=s^{'}|S_t=s] \tag {2-53}

$$

3. 马尔科夫奖励过程（Markov Reward Process）



马尔可夫奖励过程MRP，是在马尔科夫过程MP的基础上增加了衰减系数γ和奖励函数R，可以用一个四元组(S, P, R, γ)来描述。其中S={s1, s2, s3, ..., sn}，为一个状态集合，是智能体对所处环境的描述，环境的变化可以由状态的变化来表示。γ是一个折扣因子（discount factor），γ∈[0,1]。R为奖励函数或奖励信号，环境根据智能体的行为返回给智能体奖励值，智能体根据奖励值以更新策略。在奖励函数R中，为了衡量所采取的动作的有效性，奖励值是一个标量值，当数值为正数，表示智能体得到奖励，说明智能体所执行的动作使得智能体越接近目标；当数值为负数时，表示智能体得到惩罚，说明智能体所执行的动作使得智能体远离目标。奖励函数Rs表示在当前时刻t的状态St下，下一时刻t+1获得的奖励期望：

$$

R_s=E[R_{t+1}|S_t=s] \tag {2-54}

$$


4. 马尔科夫决策过程（Markov decision process）


马尔科夫决策过程MDP是带有决策的马尔可夫奖励过程MRP，马尔科夫决策过程MDP在马尔科夫奖励过程中增加了决策集合A，由五元组(S, A, P, R, γ)进行描述。A为一个动作集合，是智能体所做出的决策或采取的动作，智能体可以采取的所有动作构成了动作集合。由于智能体采取动作后会使得观测到的状态发生改变，P为状态转移概率矩阵，表示在当前时刻t的状态St下，智能体采取动作At=a之后，下一时刻t+1转移到状态St+1的概率分布，公式为：

$$

P_{ss}^a=P[S_{t+1}=s|S_t=s,A_t=a] \tag {2-55}

$$


R表示在当前时刻t的状态St下，采取动作At=a之后，下一时刻t+1获得的奖励期望：


$$

R_s^a=E[R_{t+1}|S_t=s,A_t=a] \tag {2-56}

$$


当经过一系列的探索与学习后，智能体最终能够学习到最优的行为策略，使得累积获得的奖励值最大化，当智能体得到的累积奖励值不再增加，此时奖励函数将会逐渐收敛。


5. 策略π


马尔可夫决策过程中的策略π，可以定义为智能体在特定时间特定环境下的行为方式，智能体通过策略在特定状态下选择特定的动作，它是一个概率集合（离散动作空间）或是一个分布（连续动作空间），也符合马尔科夫性。策略π表示在一个状态s下采取某个动作a的概率，公式为：

$$

\pi(a|s)=P[A_t=a|S_t=s] \tag {2-57}

$$


其中t=0, 1, 2, ... ，St∈S，S是环境状态集合，St表示时刻t的状态几何，s表示其中的某个特定的状态，At∈A(St)，A(St)是状态St下行为的集合，At代表时刻t的动作集合。因为对于每一个状态s都会有这样一个π(as)，所有状态的π(as)就形成整体策略π。


6. 值函数


奖励函数是对智能体行为的即时评价，奖励函数可以使得智能体由当前状态转移到下一个状态时的奖励值最大，是当前状态下的奖励，但并不能保证智能体到达目标时总的奖励值最大。在马尔科夫决策过程MDP中，值函数是对预期奖励的预测，针对当前状态下采取的行为，计算未来总体的累计折扣奖励（Accumulated Discounted Reward），使得智能体更注重于长远的整体利益最大化。累计折扣奖励或回报（return）Gt用来衡量在马尔科夫决策过程中，从时间步t时刻开始，所获得的所有带折扣的奖励，其中的γ是折扣因子，用来控制未来奖励对智能体当前决策的影响程度。未来状态受到当前状态s的影响逐渐衰减，γ用来控制未来奖励对智能体当前决策的影响程度。当γ=0，表示智能体仅考虑当前时刻的奖励，忽略未来所有时刻的奖励；当γ=1，表示智能体均衡地考虑当前时刻的奖励和未来时刻的奖励。回报Gt可由以下公式表示：

$$

G_t=R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty} R_{t+k+1} \tag {2-58}

$$


状态价值函数（state-value function）表示在当前状态s开始，在马尔科夫决策过程MDP中遵循策略π所能获得的期望回报，公式表达如下：

$$

v_{\pi} (s) = E_{\pi} [G_t|S_t=s,\pi] \tag {2-59}

$$


动作价值函数（action-value function）表示在马尔科夫决策过程MDP中，遵循策略π时，在给定状态s下，采取某个具体的行为a所能获得的期望回报，公式表达如下：

$$

q_{\pi}(s,a)=E_{\pi}[G_t|S_t=s|A_t=a,\pi] \tag {2-60}

$$

7. 贝尔曼方程


通过贝尔曼方程（Bellman Equation）可用下一时刻的状态值函数和及时奖励来描述当前时刻的状态函数，主要用与求解最优的值函数：

$$

\begin{align}
\begin{aligned}
v(s)&= E[G_t|S_t=s] \\
&=E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3} + ... |S_t=s] \\
&=E[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3} + ... )|S_t=s] \\
&=E[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
&=E[R_{t+1}+\gamma v(S_{t+1})|S_t=s] \\
\end{aligned} \  \tag {2-61}
\end{align}

$$


可得到状态值函数的贝尔曼期望方程的最终结果为：

$$

v_{\pi}(s)=E_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s] \tag {2-62}

$$


可得到动作值函数的贝尔曼期望方程的最终结果为：

$$

q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_t=s|A_t=a] \tag {2-63}

$$

8. 最优价值函数


最优价值函数（Optimal Value Function）可分为最优状态价值函数υ*(s)和最优动作价值函数q*(s,a)，主要用于确定马尔科夫决策过程的最优可能表现。


最优状态价值函数υ*(s)指的从所有的策略产生的状态价值函数中，选取使状态s价值最大的函数：

$$

v_*(s)=\max_{\pi} v_{\pi}(s) \tag {2-64}

$$


最优动作价值函数q*(s,a)指的是从所有策略产生的动作价值函数中，选取使状态行为对(s,a)价值最大的函数：

$$

q_*(s,a)=\max_{\pi} q_{\pi}(s,a) \tag {2-65}

$$


9. 最优策略


对于任何状态s，如果采用策略π的价值不小于采用策略π’的价值，则策略π优于策略π’：

$$
\forall S, if \quad v_{\pi}(s) \geqslant v_{\pi^{'}}(s),then \quad \pi \geqslant \pi^{'} \tag{2-66}
$$


通过最大化最优动作价值函数$q_*(s,a)$来找到最佳策略$π_*(a|s)$：

$$
\begin{align}
\pi_*(a|s)=\begin{cases}
1 \quad if \quad a={\max}\limits_{a\in A} \ q_*(s,a) \\
0 \quad otherwise
\end{cases} \tag {2-67}
\end{align}
$$

10. 贝尔曼最优方程


贝尔曼最优方程（Bellman Optimality Equation），针对最优状态价值函数$v_*$，一个状态的最优价值等于从该状态出发采取的所有行为产生的动作价值的最大值：

$$

v_*(s,a)=\max_a \ q_*(s,a) \tag {2-68}

$$


针对最优行为价值函数q*，在某个状态s下，采取某个行为的最优价值由两部分组成，一部分是离开状态s的奖励，另一部分则是所有能到达的状态s’的最优状态价值按出现概率求和：

$$

q_*(s,a)=R_s^a + \gamma \sum_{s^{'} \in S} P_{ss^{'}}^a v_*(s^{'}) \tag {2-69}

$$

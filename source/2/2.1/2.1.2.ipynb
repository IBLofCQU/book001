{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1.2 前馈神经网络得工作机制\n",
    "前面我们已经认识了单个神经元模型和多层神经网络，接下来需要探索神经网络的工作机制。我们在入手最基础的前馈神经网络前，我们需要思考以下几个问题：如何将给定的数据转化成神经网络的输入，应该怎样设计神经网络的架构，信号在神经网络中是如何计算并传播的，神经网络是如何学习并不断优化的？读者可以带着这些问题进行这一节的学习。\n",
    "\n",
    "## 1.向量和矩阵\n",
    "首先需要明确的是，在神经网络中，输入、输出数据以及网络的特征和权重都以矩阵的形式表示和存储，神经网络中的计算本质上都是对矩阵的运算。因此，了解矩阵的运算法则及意义，能够帮助我们理解深度学习中的一些基本概念和计算过程，例如前向传播和反向传播。在本节中，我们将会对向量和矩阵进行介绍，建立初步的概念，这也是后面神经网络学习的基础。\n",
    "\n",
    "**向量（Vector）**，通常用加粗的斜体小写字母或小写字母顶部加箭头来表示。只有一行元素构成的向量，称为行向量。只有一列元素构成的向量，则称为列向量。例如，一个3维列向量α可表示为：\n",
    "\n",
    "$$\n",
    "α = \\begin{bmatrix}\n",
    "a_1\\\\\n",
    "a_2 \\\\\n",
    "a_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**矩阵（Matrix）**，由m×n个数排成的m行n列数表，通常用一个加粗的斜体大写字母来表示。一个m×n矩阵，可视为由m个行向量组成，每个行向量为n维；也可视为由个n列向量构成，每个列向量为m维。特别地，当m = n时，该矩阵叫做n阶方阵。例如，一个3×2阶矩阵A可表示为：\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "2 & -4\\\\\n",
    "3 & 1 \\\\\n",
    "0 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "接着，我们将介绍在深度学习中最常用的矩阵运算法则。\n",
    "\n",
    "**矩阵加减法**：两个矩阵的加减法是将矩阵中对应的元素相加减，运算的前提是：两个矩阵需要具有相同的行和列数。下式分别举例了二阶方阵的加法和减法。\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "1+1 & 2+2\\\\\n",
    "3+3 & 4+4\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "2 & 4\\\\\n",
    "6 & 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "- \\begin{bmatrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "1-1 & 2-2\\\\\n",
    "3-3 & 4-4\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0 & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "在Python中，可以使用NumPy.array()方法生成向量或者矩阵。上述运算可转化为代码："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [6 8]]\n",
      "[[0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# 定义两个矩阵\n",
    "A = numpy.array([[1, 2], [3, 4]])\n",
    "B = numpy.array([[1, 2], [3, 4]])\n",
    "# 矩阵加减法\n",
    "C = A + B\n",
    "print(C)\n",
    "D = A - B\n",
    "print(D)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**矩阵转置**：将矩阵**A**的行换成同序数的列得到的新矩阵称为**A**的转置矩阵，记作$A^T$。例如矩阵：\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 & 0\\\\\n",
    "3 & 4 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其转置矩阵为：\n",
    "\n",
    "$$\n",
    "A^T = \\begin{bmatrix}\n",
    "1 & 3\\\\\n",
    "2 & 4\\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "在Python中，上述运算可转化为代码："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 4]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "A = numpy.array([[1, 2, 0], [3, 4, 2]])\n",
    "# T 表示转置操作 即矩阵B是矩阵A的转置矩阵\n",
    "B = A.T\n",
    "print(B)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**矩阵乘法**：设**A** = ($a_{ij}$)是一个m×s矩阵，**B** = ($b_{ij}$ )是一个s×n矩阵，那么规定矩阵**A**与矩阵**B**的乘积是一个m×n矩阵**C** = ($c_{ij}$)，其中\n",
    "\n",
    "$$\n",
    "c_{ij} = (a_{i1})(b_{1j}) + a_{i2}b_{2j} + ··· + a_{is}b_{sj} = \\sum\\limits_{k=1}^sa_{ik}b\n",
    "$$\n",
    "$$\n",
    "(i=1,2,···,m;j=1,2,···,n)\n",
    "$$\n",
    "$$\n",
    "\\tag{2 - 1}\n",
    "$$\n",
    "\n",
    "并把此乘积记作 **C=AB**。从式（2-1）可得矩阵**C**的第i行第j列的元素等于矩阵**A**的第i行的元素与矩阵**B**的第j列对应元素乘积之和。例如，\n",
    "\n",
    "$$\n",
    "C=AB=\\begin{bmatrix}\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix} · \\begin{bmatrix}\n",
    "4\\\\\n",
    "5\\\\\n",
    "6\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "32\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "我们称矩阵的乘法为点积（Dot Product）。在Python中，我们采用numpy.dot()函数完成上述运算，将运算过程转化为代码如下："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32]]\n"
     ]
    }
   ],
   "source": [
    "A = numpy.array([[1, 2, 3]])\n",
    "B = numpy.array([[4], [5], [6]])\n",
    "# dot()表示矩阵乘法\n",
    "C = numpy.dot(A,B)\n",
    "print(C)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "必须注意：矩阵乘法是不可交换的（即**AB≠BA**）。而且只有当第一个矩阵的列数和第二个矩阵的行数相同时，两个矩阵才能相乘。虽然矩阵乘法是人为的规则，但它大大简化了计算的表达，可以将巨大的计算量很简洁地表达出来，这一点对机器学习算法的开发和使用有重要的作用。与此同时，GPU 的设计就是源于矩阵计算处理的基本概念，GPU 会并行地操作整个矩阵里的元素，而不是一个一个地串行处理，提高了计算速度。\n",
    "\n",
    "上面介绍的矩阵乘法又被称作矩阵的内积（Inner Product），还有另一种矩阵乘法运算——外积（Outer Product），用符号$\\bigotimes$表示。假设矩阵A = ($a_{ij}$)、B = ($b_{ij}$) 分别为两个2×2的矩阵，则的A$\\bigotimes$B计算可以表示为：\n",
    "\n",
    "$$\n",
    "A \\bigotimes B=\\begin{bmatrix}\n",
    "a_{11}B & a_{12}B \\\\\n",
    "a_{21}B & a_{22}B\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\begin{matrix}\n",
    "a_{11}b_{11} & a_{11}b_{12} \\\\\n",
    "a_{11}b_{21} & a_{11}b_{22} \\\\\n",
    "\\end{matrix}& \\begin{matrix}\n",
    "a_{12}b_{11} & a_{12}b_{12} \\\\\n",
    "a_{12}b_{21} & a_{12}b_{22} \\\\\n",
    "\\end{matrix} \\\\\n",
    "\\begin{matrix}\n",
    "a_{21}b_{11} & a_{21}b_{12} \\\\\n",
    "a_{21}b_{21} & a_{21}b_{22} \\\\\n",
    "\\end{matrix} & \\begin{matrix}\n",
    "a_{22}b_{11} & a_{22}b_{12} \\\\\n",
    "a_{22}b_{21} & a_{22}b_{22} \\\\\n",
    "\\end{matrix}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\tag{2-2}\n",
    "$$\n",
    "\n",
    "\n",
    "除此之外，**Hadamard**积也是我们常用的一种矩阵乘法运算，用符号$\\bigodot$表示。Hadamard积要求参与运算的两个矩阵形状相同，然后对两个矩阵对应位置上的元素相乘，并产生相同维度的第三个矩阵。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "假设矩阵A = ($a_{ij}$)、B = ($b_{ij}$) 分别为两个3×2矩阵，则A与B的Hadamard积结果C也是一个3×2矩阵，可表示为：\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "A \\bigodot B = \\begin{bmatrix}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\\\\\n",
    "a_{31} & a_{32}\\\\\n",
    "\\end{bmatrix} \\bigodot \\begin{bmatrix}\n",
    "b_{11} & b_{12}\\\\\n",
    "b_{21} & b_{22}\\\\\n",
    "b_{31} & b_{32}\\\\\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "a_{11}b_{11} & a_{12}b_{12} \\\\\n",
    "a_{21}b_{21} & a_{22}b_{22} \\\\\n",
    "a_{31}b_{31} & a_{32}b_{32} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\tag{2-3}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "矩阵在神经网络中的应用十分广泛。例如，神经网络中的神经元，需要接收来自其他神经元的信号。这些信号来源不同，对神经元的影响也不同。需要给每个输入信号分配一个权重，神经元将对接收到的信号进行加权求和作为其输入。整个过程如果用数学公式一一表示将会非常复杂，在后续的学习中，我们会更加深入地体会到矩阵在简化神经网络表达中的重要作用。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.激活函数\n",
    "从上一小节中，我们知道了前馈神经网络中每个神经元对于输入数据x的运算可以由下式表示：\n",
    "$$\n",
    "o=A(wx+b)\n",
    "$$\n",
    "$$\n",
    "\\tag{2-4}\n",
    "$$\n",
    "\n",
    "其中，A是神经元使用的激活函数，w为神经元的权重（Weights），b是偏置（Bias），它们是神经网络中可通过不断训练而学习到的参数。\n",
    "\n",
    "很显然，如果不使用激活函数，那么每一个神经元中都只是对输入数据进行了简单的线性变换，每一层输出都是上一层输入的线性函数，这样，即使网络的深度再深，输出值都只是输入值的线性组合。而激活函数则为神经元引入了非线性因素，这样神经网络就具有了更强大的表达能力和拟合能力。\n",
    "\n",
    "为了更好地理解什么是激活函数，我们用一个简单的阶跃函数来说明。\n",
    "\n",
    "![图 2-4 阶跃函数](../../_static/2/2.1/2-4.png)\n",
    "\n",
    "如上图所示，在输入值小于阈值时，函数的输出为0。一旦输入达到设定的阈值，输出就一跃而起。具有这种行为的人工神经元就像一个真正的生物神经元，只有当输入信号达到了阈值，这个神经元才会激活，处于兴奋状态；否则这个神经元将被抑制。但是阶跃函数的缺点也是非常明显的。首先，它是不连续且不光滑的，这就导致在反向传播时这一层很难学习。其次，阶跃函数“非黑即白”的特性虽然可能最符合生物神经网络，但是实际中有时我们需要表达“20%被激活”这种概念，即我们需要一个模拟的激活值而非简单的“0”或“1”。\n",
    "\n",
    "因此，我们需要改善激活函数使其具备如下性质：\n",
    "\n",
    "激活函数应是连续并可导（允许少数点上不可导）的非线性函数。\n",
    "激活函数及其导函数要尽可能的简单，这有利于提高网络的计算效率。\n",
    "激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。\n",
    "\n",
    "总而言之，激活函数对神经网络而言非常重要，它通过为神经元设定阈值，使得网络中各层次的输入数据始终存在于某一范围之内，很大程度上影响着模型的学习效果及效率。下面介绍几种神经网络中常用的激活函数：\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1)Sigmoid函数\n",
    "\n",
    "Sigmoid函数也叫Logistic函数，在逻辑回归、人工神经网络中有着广泛的应用。Sigmoid激活函数和导函数的表达式为：\n",
    "\n",
    "$$\n",
    "Sigmoid(x) =  \\dfrac {1} {1+e^{-x}}\n",
    "$$\n",
    "$$\n",
    "\\tag{2-5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Sigmoid^{'}(x) =  \\dfrac {e^{-x}} {(1+e^{-x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tag{2-6}\n",
    "$$\n",
    "从下图可知，Sigmoid函数和阶跃函数非常相似，但是解决了阶跃函数不光滑和不连续的问题，同时它还成功引入了非线性。Sigmoid函数能够把输入的连续实值变换为0和1之间的输出，且在0.5处为中心对称，并且越靠近x = 0的取值斜率越大。由于Sigmoid函数值域处在(0,1)，所以往往被用到二分类任务的输出层，可以将它的输出值看作数据归于某类的概率，从而达到预测类别的目的。\n",
    "\n",
    "Sigmoid的局限之处在于，其梯度范围在0-0.25之间，当输入值大于4或者小于-4时，它的梯度就非常接近0了。在深层网络中，这非常容易造成“梯度消失”，使网络难以训练。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(2)tanh双曲正切函数\n",
    "\n",
    "tanh双曲正切函数是由双曲正弦和双曲余弦这两个基本双曲函数推导而来，其关于原点中心对称。tanh函数和导函数的表达式为：\n",
    "\n",
    "$$\n",
    "tanh(x) = \\dfrac {e^x - e^{-x}} { e^x + e^{-x} }  \\tag{2-7}\n",
    "$$\n",
    "\n",
    "$$\n",
    "tanh^{'}(x)=1-(tanh(x))^2=\\dfrac {4e^{-2x}} {(1+e^{-2x})^2} \\tag{2-8}\n",
    "$$\n",
    "\n",
    "从图2-5可以看出，与Sigmoid函数相比，tanh的梯度范围更广（0-1），能一定程度上缓解梯度消失的问题的发生。除此之外，tanh的输出均值是0，这对于神经网络的学习而言意义重大，在后面的反向传播部分我们将对此进行解释。但是tanh仍不能完全避免梯度消失的问题。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![图 2-5 Sigmoid 函数,tanh函数及其导函数图像对比](../../_static/2/2.1/2-5.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

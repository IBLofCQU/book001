# 使用不同学习率

学习率是训练神经网络最重要的超参数之一。学习率决定了每次更新模型参数时，模型参数的变化程度。学习率越大，更新后的模型参数变化程度越大，模型的学习速度越快。学习率越小，更新后的模型参数变化程度越小，模型的学习速度越慢。如果学习率太大，参数更新程度大，可能会直接越过最优参数点，模型难以收敛。如果学习率太小，参数更新程度小，可能需要很长时间才能到达最优参数点。在训练DCGAN的生成器和判别器时，我们对这两个网络使用相同的学习率。但一些研究发现，判别器和生成器训练的难易程度不同，若训练时对两者使用相同的学习率，可能会导致两个网络收敛速度不同，难以使两个网络性能达到比较均衡的状态。

我们尝试对判别器和生成器使用不同的学习率，即采用TTUR（Two Timescale Update Rule）方法，看看能否使DCGAN生成的建筑立面效果更好。在优化生成器时，我们通常假设判别器对生成样本和真实样本的判别结果是正确的，因为这样才能指导生成器更新参数。基于这个假设，我们为判别器设置一个较大的学习率，让判别器更快收敛，以便判别器指导生成器向更好的方向更新参数。具体的代码如下所示，完整代码可扫描二维码下载。

:::{todo}
待适配
:::


:::{literalinclude} ../codes/chapter_4_3_2_01.py
:caption: chapter_4_3_2_01.py
:language: python
:linenos:
:::

对判别器与生成器采用不同学习率的训练时间约为191分钟，训练结果为图4-10右图，图4-10左图为未使用改进方法的训练结果。尝试改进方法后，一些样本效果较好，但仍有小部分生成样本质量不高。读者可以尝试对生成器使用较大的学习率，对判别器使用较小的学习率，以此验证更多训练的可能性。

:::::{grid} 2 2 2 2

::::{grid-item}
:::{figure} ../../_static/4/4.3/4-10-a.png
:::
::::
::::{grid-item}
:::{figure} ../../_static/4/4.3/4-10-b.png
:::
::::
:::::
<div class="show-mid">图4-10 改进方法前后的建筑立面效果对比</div>
<br>
<br>

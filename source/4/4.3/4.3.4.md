# 最小二乘损失

DCGAN主要是对生成器和判别器的架构进行改进，利用一些先进模块的优越性能提升了模型性能。但在生成对抗网络的学习过程中，需要着重关注对生成器与判别器性能的平衡，不能一强一弱。而平衡生成器与判别器强弱的关键部分就是损失函数。

DCGAN使用的交叉熵损失函数会使被判别器判别为真但仍远离真实数据的生成样本停止迭代，因为这些生成样本已经成功欺骗了判别器，更新生成器时便会出现梯度消失的问题。换句话说，因为判别器已经对样本进行了正确的分类，此时的损失已经很小，判别器产生的梯度也非常小，故在后续的训练过程中，几乎不会再对这部分样本的模型参数进行更好的更新。

最小二乘损失函数能够惩罚距离决策边界太远的生成样本。因为要使最小二乘损失更小，需将距离决策边界太远的生成样本拉向决策边界。随着模型的不断训练，生成样本便会更趋近于真实样本。因此我们尝试将DCGAN的交叉熵损失函数替换为最小二乘损失函数，看看是否有改进效果。两种损失函数的决策行为如图4-12所示。

:::{figure-md}
<img src="../../_static/4/4.3/4-11-a.png">

图4-12 两种损失函数的决策行为示意图
:::

图4-12中蓝色线为交叉熵损失函数的决策边界，红色线为最小二乘损失函数的决策边界。决策边界是一种划分空间的分割界面，决策边界同侧的所有数据点同属于同一类别。判别器就起到划分样本数据所属类别的决策作用。可以看到，左图中有许多距离真实样本较远的离群点，这部分样本很难再向决策边界靠近。若使用最小二乘损失函数进行训练，随着模型的迭代，距离决策边界较远的生成样本会被逐步拉向真实样本，使这些生成样本更加接近真实样本。
最小二乘损失函数的表达式如下：


